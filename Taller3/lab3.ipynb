{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab III - Advanced Topics\n",
    "\n",
    "## Machine Learning II\n",
    "\n",
    "* Andr√©s Casta√±o Licona\n",
    "* Eileen Melissa Arevalo Garnica\n",
    "* Mois√©s Alfonso Guerrero Jim√©nez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workshop III\n",
    "\n",
    "1. In your own words, describe what vector embeddings are and what they are useful for.\n",
    "\n",
    "**R/** Embeddings are vector representations of words in an N-dimensional space, where it is possible to represent them in a lower dimension than if they were represented in one-hot encoding, which assigns a dimension per word. These embeddings capture semantic and syntactic relationships between words, allowing NLP and machine learning algorithms to work more effectively in tasks such as sentiment analysis and automatic translation, among others. It is important to highlight that embeddings are not specifically used as a final part of the process, but rather as an intermediate step in the application of NLP models, where they are adjusted and refined with other parameters during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What do you think is the best distance criterion to estimate how far two embeddings (vectors) are from each other? Why?\n",
    "\n",
    "**R/** The best distance criterion to estimate how far are two embeddings from each other is the cosine similarity. This is a measure that instead of calculating the distance between the 2 points as is done with the Euclidean distance, it evaluates the angle between the two vectors.\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* Cosine similarity is simple and only requires the dot product of the vectors and the multiplication of their magnitudes. This simplicity leads to efficient calculations, making it suitable for real-time applications and large data sets.\n",
    "* Unlike other distance-based similarity measures, cosine similarity considers the angle between vectors, which provides a more intuitive sense of similarity. Smaller angles indicate greater similarity and the measurement ranges between -1 and 1, making it easier to interpret.\n",
    "* Cosine similarity is scale-invariant, meaning that it is not affected by the magnitudes of the vectors. This is especially useful in scenarios where you want to focus solely on the directionality of the vectors, rather than their length. Whether the values in your vector are in the tens or the millions, the cosine similarity will remain the same, making it versatile across different scales.\n",
    "\n",
    "**References**\n",
    "\n",
    "https://www.datastax.com/guides/what-is-cosine-similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let us build a Q&A (question answering) system! üòÄFor this, consider the following steps:\n",
    "   - Pick whatever text you like, in the order of 20+ paragraphs.\n",
    "   - Split that text into meaningful chunks/pieces.\n",
    "   - Implement the embedding generation logic. Which tools and approaches would help you generate them easily and high-level?\n",
    "   - For every question asked by the user, return a sorted list of the N chunks/pieces in your text that relate the most to the question. Do results make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**R/** The selected text taken from the BBC: **El plan de EE.UU. para que se volviera a emitir la telenovela venezolana \"Kassandra\" en Bosnia ante el temor de que estallara de nuevo la guerra** (https://www.bbc.com/mundo/articles/c6pd6y37vd8o)\n",
    "\n",
    "The text was splitted into paragraphs using the new line separator (\\n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The embedding logic was implemented using the `GIST-Embedding-v0` model developed by avsolatorio and published in Hugging face (https://huggingface.co/avsolatorio/GIST-Embedding-v0). This model \"introduces a novel strategy that enhances in-batch negative selection during contrastive training through a guide model\" (https://arxiv.org/pdf/2402.16829.pdf). This approach breaks away from using random sampling and the flawed assumption that all negative samples within a batch are equally valuable. This significantly reduces the impact of noise caused by data quality issues and improves the process of fine-tuning the model.\n",
    "\n",
    "The implementetion logic is very simple:\n",
    "1. Call the model.\n",
    "2. Merge both the question and the split text into a single list (It is necessary to put the question into the first position in the created list).\n",
    "3. Create the embedding using the `enconde` function from the model.\n",
    "4. Calculate the scores of the embeddings using a cosine similarity and sort them descending.\n",
    "5. Select the N chunks with the major scores. In our case we select the three most relevant scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T00:31:51.890561Z",
     "start_time": "2024-02-29T00:31:44.053291Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "EMBEDDING_MODEL = SentenceTransformer(\"avsolatorio/GIST-Embedding-v0\")\n",
    "\n",
    "def split_text_by_paragraph(filename):\n",
    "    \"\"\"\n",
    "    Opens a text file and splits its content by new line mark.\n",
    "\n",
    "    Args:\n",
    "        filename: The name of the text file to open.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings, where each string is a sentence from the text file\n",
    "        ending with a dot.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        text_ = file.read()\n",
    "    sentences = text_.split(\"\\n\")\n",
    "    return [sentence for sentence in sentences if sentence.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T00:32:38.060123Z",
     "start_time": "2024-02-29T00:32:38.000733Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the example text\n",
    "TEXT_BASE = split_text_by_paragraph('http_question_answering/question_answering/q_a/ml_models/texto_base.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T00:51:59.891724Z",
     "start_time": "2024-02-29T00:51:59.886612Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "\n",
    "\n",
    "def get_most_related_paragraphs(model, text: list[str], question:str, number_of_chunks: int = 3):\n",
    "    \"\"\"\n",
    "    This function takes a text split into some text logic units and, base on a question, \n",
    "    return the units most related with the question\n",
    "    :param model: The embedding model\n",
    "    :param text: the text split into text units\n",
    "    :param question: the question to be answered\n",
    "    :param number_of_chunks: the num of chunks returned\n",
    "    :return: \n",
    "    a numpy array with the most related (relative to the question) units from the text\n",
    "    \"\"\"\n",
    "    question = [question]\n",
    "    full_text = question + text\n",
    "    # calculate the embedding of the text\n",
    "    embeddings = model.encode(full_text, convert_to_tensor=True)\n",
    "    # calculates the scores of the question vs the paragraphs and sort them\n",
    "    scores = util.cos_sim(embeddings[:1], embeddings[1:]) * 100\n",
    "    \n",
    "    order = np.argsort(scores.tolist()[0])[::-1].astype(int)\n",
    "    # return the most related paragraphs\n",
    "    text = np.array(text)\n",
    "    if number_of_chunks <= len(text):\n",
    "        return text[order[:number_of_chunks]]\n",
    "    return text[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T00:52:05.499389Z",
     "start_time": "2024-02-29T00:52:03.768847Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Esta joven, que hab√≠a sido Gisela -interpretada por Rebeca Gonz√°lez- en la versi√≥n original de 1973 llamada \"Peregrina\", y que luego fuera Raiza (Catherine Fulop) en \"La muchacha del circo\" de 1988, volvi√≥ bajo el nombre de Kassandra interpretada por Coraima Torres.',\n",
       "       '\"Comenc√© a hablar con un tipo que me dijo: \\'Escuche, ni siquiera puedo decirle mi nombre en este momento. Hay un canal de televisi√≥n p√∫blico y usted tiene un programa que est√° vendiendo en esta zona. Realmente lo queremos mantener en el aire porque fue retirado y la guerra se ha intensificado debido a ello, as√≠ que necesitamos su ayuda para volver a ponerlo en el aire\\'\", relat√≥.',\n",
       "       '\"Se enamoraron de la historia y creo que les trajo paz y una sensaci√≥n de normalidad. La historia de la pobreza a la riqueza siempre atrapa a la gente y ver a una persona como Coraima que tuvo muchas dificultades en sus primeros a√±os, que la trataron muy mal y resucit√≥ y se convirti√≥ en reina, es hermoso, porque da esperanza a la gente. Me siento bendecido de haber podido ayudar un poquito y siento que si pudimos contribuir simplemente salvando algunas vidas. Est√°bamos felices de poder hacerlo\", afirm√≥ por su parte P√°ez.'],\n",
       "      dtype='<U526')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of use\n",
    "question = '¬øA qui√©n interpret√≥ Rebeca Gonz√°lez?'\n",
    "get_most_related_paragraphs(EMBEDDING_MODEL, TEXT_BASE, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. What do you think that could make these types of systems more robust in terms of semantics and functionality?\n",
    "\n",
    "**Ways to make these systems more robust**\n",
    "\n",
    "* In order to improve the result of the model for selecting the most relevant paragraphs that could answer the question, we could establish a minimum threshold for cosine similarity score to be taken into account.\n",
    "* Using advanced text representations like BERT which encodes text from different languages capturing semantic similarities between sentences more effectively than traditional representations.\n",
    "* Another way to make these models more robust is the integration of semantic lexicons, that is, incorporating semantic properties of the words and the complex relationships between them.\n",
    "\n",
    "There are databases that could be used to take into account the semantic relationships between words like:\n",
    "\n",
    "* **WordNet** is a linguistic database built by humans that contains grouped synonyms of nouns, verbs, adjectives and adverbs in English, along with semantic relationships such as synonyms, hypernyms, hyponyms, meronyms, among others.\n",
    "* **FrameNet** is a lexicon that relates phrases/sentences (semantic frames) with individual words (lexical units), providing examples that show their meaning and use.\n",
    "\n",
    "**References**\n",
    "\n",
    "https://www-sciencedirect-com.udea.lookproxy.com/science/article/pii/S0306457322000450 \n",
    "\n",
    "https://www-sciencedirect-com.udea.lookproxy.com/science/article/pii/S0950705120300903"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Bonus points if deployed on a local or cloud server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T00:43:31.875115Z",
     "start_time": "2024-02-29T00:42:51.965867Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watching for file changes with StatReloader\r\n",
      "Performing system checks...\r\n",
      "\r\n",
      "System check identified no issues (0 silenced).\r\n",
      "February 29, 2024 - 00:43:03\r\n",
      "Django version 4.2.10, using settings 'question_answering.settings'\r\n",
      "Starting development server at http://127.0.0.1:8000/\r\n",
      "Quit the server with CONTROL-C.\r\n",
      "\r\n",
      "[29/Feb/2024 00:43:13] \u001b[m\"GET / HTTP/1.1\" 200 15743\u001b[0m\r\n",
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "# Implementation of the http server\n",
    "!python http_question_answering/question_answering/manage.py runserver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
